{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Let's try to play with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0471333d6a446d85bafd0807c5de99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1378033), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import tqdm.notebook as tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "f = open(\"../data/goodreads_reviews_spoiler.json\")\n",
    "lines = []\n",
    "for i in tqdm.tqdm(range(1378033)):\n",
    "    lines.append(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "len(lines)\n",
    "random.shuffle(lines)\n",
    "test_size = int(len(lines)*0.2)\n",
    "test_lines = lines[:test_size]\n",
    "valid_lines = lines[test_size:test_size+10000]\n",
    "train_lines = lines[test_size+10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '8842281e1d1347389f2ab93d60773d4d',\n",
       " 'timestamp': '2017-08-30',\n",
       " 'review_sentences': [[0, 'This is a special book.'],\n",
       "  [0,\n",
       "   'It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind.'],\n",
       "  [0,\n",
       "   'This is what I love about good science fiction - it pushes your thinking about where things can go.'],\n",
       "  [0,\n",
       "   \"It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read.\"],\n",
       "  [0,\n",
       "   'For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc.'],\n",
       "  [0, 'It is a book about science, and aliens.'],\n",
       "  [0,\n",
       "   'The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell.'],\n",
       "  [1,\n",
       "   'Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though.'],\n",
       "  [1,\n",
       "   'But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned?'],\n",
       "  [1,\n",
       "   'That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together.'],\n",
       "  [1,\n",
       "   'I did like how the book unveiled the Trisolaran culture through the game.'],\n",
       "  [1,\n",
       "   \"It was a smart way to build empathy with them and also understand what they've gone through across so many centuries.\"],\n",
       "  [1, 'And who know a 3 body problem was an unsolvable math problem?'],\n",
       "  [1,\n",
       "   \"But I still don't get who made the game - maybe that will come in the next book.\"],\n",
       "  [1, 'I loved this quote:'],\n",
       "  [1,\n",
       "   '\"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists?'],\n",
       "  [1, 'How many neutrons and electrons?'],\n",
       "  [1, 'Probably no fewer than a hundred million.'],\n",
       "  [1,\n",
       "   'Every collision was probably the end of the civilizations and intelligences in a microcosmos.'],\n",
       "  [1,\n",
       "   'In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons.'],\n",
       "  [1,\n",
       "   'Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"']],\n",
       " 'rating': 5,\n",
       " 'has_spoiler': True,\n",
       " 'book_id': '18245960',\n",
       " 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89627, 0.06503980673902585)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_spoiler_reviews = np.sum([line['has_spoiler'] for line in lines])\n",
    "n_spoiler_reviews, n_spoiler_reviews/len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569724, 17672655, 0.03223760097167064)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_spoiler_sentences = np.sum([sentence[0] for line in lines for sentence in line['review_sentences']])\n",
    "n_sentences = np.sum([len(line['review_sentences']) for line in lines])\n",
    "n_spoiler_sentences, n_sentences, n_spoiler_sentences/n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence[1] for line in lines for sentence in line['review_sentences']]\n",
    "labels = [sentence[0] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a353b573964441283b22b1fc22c5642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1378033), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def clean_review(sentence):\n",
    "    return ''.join([c for c in sentence.lower() if c not in punctuation])\n",
    "\n",
    "words = defaultdict(int)\n",
    "    \n",
    "for line in tqdm.tqdm(lines):\n",
    "    for sentence in line['review_sentences']:\n",
    "        sentence = clean_review(sentence[1])\n",
    "        if (sentence != '') and (sentence is not None):\n",
    "            for word in sentence.split():\n",
    "                words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stem these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b4c562fb0047148a5add72915019dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=895246), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stem_words = defaultdict(int)\n",
    "map_stem_words = {}\n",
    "for word in tqdm.tqdm(list(words.keys())):\n",
    "    stem_word = stemmer.stem(word)\n",
    "    map_stem_words[word] = stem_word\n",
    "    stem_words[stem_word] += words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some high-frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12689322, 'the'),\n",
       " (8411444, 'and'),\n",
       " (7832234, 'i'),\n",
       " (6832296, 'to'),\n",
       " (6524879, 'a'),\n",
       " (5653305, 'of'),\n",
       " (5082446, 'it'),\n",
       " (3898974, 'that'),\n",
       " (3687311, 'is'),\n",
       " (3661654, 'in'),\n",
       " (3514977, 'wa'),\n",
       " (3468597, 'thi'),\n",
       " (3132365, 'book'),\n",
       " (2491119, 'but'),\n",
       " (2272462, 'for'),\n",
       " (2082090, 'her'),\n",
       " (2049041, 'with'),\n",
       " (1789251, 'she'),\n",
       " (1624647, 'read'),\n",
       " (1615469, 'be')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [(stem_words[w], w) for w in stem_words]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "counts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Basic Test for Whole Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.1 Get the most 10000 popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bags = counts[:10000]\n",
    "popular_words = set([word[1]for word in word_bags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.2 Merge Sentences into Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f132edc7e4244a6b9dd1868bc5220621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1378033), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = defaultdict(set)\n",
    "for line in tqdm.tqdm(lines):\n",
    "    book_id = line['book_id']\n",
    "    sentences = line['review_sentences']\n",
    "    label = line['has_spoiler']\n",
    "    paragraph = ''\n",
    "    for sentence in sentences:\n",
    "        paragraph = paragraph + \" \" + clean_review(sentence[1])\n",
    "    reviews[book_id].add((paragraph, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split each paragraph into list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3428013754e548f8a3b8e277142d401f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "splitted_review = defaultdict(set)\n",
    "for book_id in tqdm.tqdm(reviews):\n",
    "    for paragraph, label in reviews[book_id]:\n",
    "        new_paragraph = []\n",
    "        for word in paragraph.split():\n",
    "            new_paragraph.append(map_stem_words[word])\n",
    "        splitted_review[book_id].add((\" \".join(new_paragraph), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.3 Let's compute Document Frequency (DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823d5e13b95c427fac1c708dc3bf9e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DF = defaultdict(int)\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    for paragraph, _ in splitted_review[book_id]:\n",
    "        all_words = set(paragraph.split())\n",
    "        for word in all_words:\n",
    "            if word in popular_words:\n",
    "                DF[word+\"-\"+book_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056835b65ac1409f9a12930f40f0c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF = defaultdict(float)\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    d_i = len(splitted_review[book_id])\n",
    "    for word in popular_words:\n",
    "        if (word+\"-\"+book_id) in DF:\n",
    "            final_DF[word+\"-\"+book_id] = DF[word+\"-\"+book_id] / d_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = final_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.4 Let's compute Inverse Item Frequency (IIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4757d6aa3c8f45eda1d24cc885382fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IF = defaultdict(float)\n",
    "epsilon = 1e-5\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    whole_paragraph = ''\n",
    "    for paragraph, _ in splitted_review[book_id]:\n",
    "        whole_paragraph += ' ' + paragraph\n",
    "    all_words = set(whole_paragraph.split())\n",
    "    for word in all_words:\n",
    "        if word in popular_words:\n",
    "            IF[word] += 1\n",
    "\n",
    "IIF = defaultdict(float)    \n",
    "for word in IF:\n",
    "    IIF[word] = np.log((IF[word] + epsilon) / (len(splitted_review) + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.5 Let's compute DF-IIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0380dbc7c0442e4917f3ed2591a57c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29864070), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DF_IIF = defaultdict(float)\n",
    "for word_book_id in tqdm.tqdm(DF):\n",
    "    try:\n",
    "        word, book_id = word_book_id.split(\"-\")\n",
    "    except:\n",
    "        print(word_book_id)\n",
    "    DF_IIF[word_book_id] = DF[word_book_id] * IIF[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.6 Save Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/DF-IIF.pkl', 'wb+') as f:\n",
    "    pickle.dump({'DF':DF, 'IIF':IIF, 'DF-IIF':DF_IIF}, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Let's do some basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute IIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurance_word_item = defaultdict(set())\n",
    "for line in tqdm.tqdm(lines):\n",
    "    for sentence in line['review_sentences']:\n",
    "        occur_words = {}\n",
    "        sentence = clean_review(sentence[1])\n",
    "        if (sentence != '') and (sentence is not None):\n",
    "            for word in sentence.split():\n",
    "                occurance_word_item[line['book_id']].add(map_stem_words[occur_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute DF-IIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
