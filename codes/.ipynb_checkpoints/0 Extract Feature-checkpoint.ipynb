{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca04e380ee4f0a8ba80e366a751813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1378033), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import tqdm.notebook as tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "f = open(\"../data/goodreads_reviews_spoiler.json\")\n",
    "lines = []\n",
    "for i in tqdm.tqdm(range(1378033)):\n",
    "    lines.append(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "len(lines)\n",
    "random.shuffle(lines)\n",
    "test_size = int(len(lines)*0.2)\n",
    "test_lines = lines[:test_size]\n",
    "valid_lines = lines[test_size:test_size+10000]\n",
    "train_lines = lines[test_size+10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def clean_review(sentence):\n",
    "    return ''.join([c for c in sentence.lower() if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shaor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a301cd48d0440eb72ffdaa871acc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1092427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "words = defaultdict(int)\n",
    "\n",
    "for line in tqdm.tqdm(train_lines):\n",
    "    for sentence in line['review_sentences']:\n",
    "        sentence = clean_review(sentence[1])\n",
    "        if (sentence != '') and (sentence is not None):\n",
    "            for word in sentence.split():\n",
    "                if word not in cachedStopWords:\n",
    "                    words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stem the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35942b69d8794b2989c8d101191cda67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=765953), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stem_words = defaultdict(int)\n",
    "map_stem_words = {}\n",
    "for word in tqdm.tqdm(list(words.keys())):\n",
    "    stem_word = stemmer.stem(word)\n",
    "    map_stem_words[word] = stem_word\n",
    "    stem_words[stem_word] += words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most popular 500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(stem_words[w], w) for w in stem_words]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "word_bags = counts[:500]\n",
    "popular_words = set([word[1]for word in word_bags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute splitted reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0441164cf5a40dfb4b7bb22b85df0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1092427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = defaultdict(set)\n",
    "for line in tqdm.tqdm(train_lines):\n",
    "    book_id = line['book_id']\n",
    "    sentences = line['review_sentences']\n",
    "    label = line['has_spoiler']\n",
    "    paragraph = ''\n",
    "    for sentence in sentences:\n",
    "        paragraph = paragraph + \" \" + clean_review(sentence[1])\n",
    "    reviews[book_id].add((paragraph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacdac0e0d734b26854ab23e333f288b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "splitted_review = defaultdict(set)\n",
    "for book_id in tqdm.tqdm(reviews):\n",
    "    for paragraph, label in reviews[book_id]:\n",
    "        new_paragraph = []\n",
    "        for word in paragraph.split():\n",
    "            if word not in cachedStopWords:\n",
    "                new_paragraph.append(map_stem_words[word])\n",
    "        splitted_review[book_id].add((\" \".join(new_paragraph), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Extract Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 Extract DF-IIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eea5211de274713bd4f5f51e5b2bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45634e1f35874efa9c63a0b7fff9f958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_word_in_book = defaultdict(int)\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    for paragraph, _ in splitted_review[book_id]:\n",
    "        all_words = set(paragraph.split())\n",
    "        for word in all_words:\n",
    "            if word in popular_words:\n",
    "                count_word_in_book[word+\"-\"+book_id] += 1\n",
    "\n",
    "DF = defaultdict(float)\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    d_i = len(splitted_review[book_id])\n",
    "    for word in popular_words:\n",
    "        if (word+\"-\"+book_id) in count_word_in_book:\n",
    "            DF[word+\"-\"+book_id] = count_word_in_book[word+\"-\"+book_id] / d_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute IIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca05788605894557aba64ffb21f33913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "IF = defaultdict(float)\n",
    "epsilon = 1e-5\n",
    "for book_id in tqdm.tqdm(splitted_review):\n",
    "    whole_paragraph = ''\n",
    "    for paragraph, _ in splitted_review[book_id]:\n",
    "        whole_paragraph += ' ' + paragraph\n",
    "    all_words = set(whole_paragraph.split())\n",
    "    for word in all_words:\n",
    "        if word in popular_words:\n",
    "            IF[word] += 1\n",
    "\n",
    "IIF = defaultdict(float)    \n",
    "for word in IF:\n",
    "    IIF[word] = - np.log((IF[word] + epsilon) / (len(splitted_review) + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute DF-IIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e721d5110a4bc0b9ffeae93f6d341d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8511794), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DF_IIF = defaultdict(float)\n",
    "for word_book_id in tqdm.tqdm(DF):\n",
    "    word, book_id = word_book_id.split(\"-\")\n",
    "    DF_IIF[word_book_id] = DF[word_book_id] * IIF[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60860584a9af412c883212b006babba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1092427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "popular_words = list(popular_words)\n",
    "\n",
    "def feat_df_iif(words, book_id):\n",
    "    feat = [0] * 500\n",
    "    for word in words:\n",
    "        if word in map_stem_words:\n",
    "            word = map_stem_words[word]\n",
    "        else:\n",
    "            continue\n",
    "        if word in popular_words:\n",
    "            word_id = popular_words.index(word)\n",
    "            feat[word_id] += DF_IIF[word+'-'+book_id]\n",
    "    feat = feat + [1]\n",
    "    return feat\n",
    "\n",
    "def get_data_and_label(target_lines, feat):\n",
    "    target_data = []\n",
    "    target_label = []\n",
    "    for line in tqdm.tqdm(target_lines):\n",
    "        book_id = line['book_id']\n",
    "        label = line['has_spoiler']\n",
    "        whole_review = ''\n",
    "        for sentence in line['review_sentences']:\n",
    "            whole_review = whole_review + ' ' + clean_review(sentence[1])\n",
    "        target_data.append(feat(whole_review.split(), book_id))\n",
    "        target_label.append(label)\n",
    "    return target_data, target_label\n",
    "\n",
    "train_data, train_label = get_data_and_label(train_lines, feat_df_iif)\n",
    "valid_data, valid_label = get_data_and_label(valid_lines, feat_df_iif)\n",
    "test_data, test_label = get_data_and_label(test_lines, feat_df_iif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/DF-IIF.json', 'w+') as f:\n",
    "    json.dump({'DF':DF, 'IIF':IIF, 'DF-IIF':DF_IIF}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
